{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Iceberg Metadata Layers with PySpark\n",
    "## A Complete Practical Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 39780)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import json\n",
    "\n",
    "# Initialize Spark with Iceberg configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IcebergMetadataExploration\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.local.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.local.warehouse\", \"/home/jovyan/iceberg/warehouse\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Sample Iceberg Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create database and partitioned table\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS local.db\")\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS local.db.sample\")\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE local.db.sample (\n",
    "    id bigint,\n",
    "    data string,\n",
    "    dt date)\n",
    "USING iceberg\n",
    "PARTITIONED BY (months(dt))\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Insert Data with Proper Date Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import datetime\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Explicit schema\n",
    "schema = StructType([\n",
    "    StructField(\"id\", LongType()),\n",
    "    StructField(\"data\", StringType()),\n",
    "    StructField(\"dt\", DateType())\n",
    "])\n",
    "\n",
    "# Convert dt to datetime.date\n",
    "data = [\n",
    "    (1, \"A\", datetime.date(2023, 1, 1)),\n",
    "    (2, \"B\", datetime.date(2023, 1, 15)),\n",
    "    (3, \"C\", datetime.date(2023, 2, 1))\n",
    "]\n",
    "\n",
    "# Create DataFrame directly with DateType\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Now you can write to Iceberg\n",
    "df.writeTo(\"local.db.sample\").append()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. View Table Metadata Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table stored at: /home/jovyan/iceberg/warehouse/db/sample\n"
     ]
    }
   ],
   "source": [
    "# Get table location\n",
    "table_info = spark.sql(\"DESCRIBE EXTENDED local.db.sample\").collect()\n",
    "location = [row.data_type for row in table_info if row.col_name == \"Location\"][0]\n",
    "print(f\"Table stored at: {location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Explore Metadata Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata versions:\n",
      "+-----------------------+------------------------------------------------------------------+\n",
      "|timestamp              |file                                                              |\n",
      "+-----------------------+------------------------------------------------------------------+\n",
      "|2025-07-12 06:04:04.675|/home/jovyan/iceberg/warehouse/db/sample/metadata/v1.metadata.json|\n",
      "|2025-07-12 06:04:11.028|/home/jovyan/iceberg/warehouse/db/sample/metadata/v2.metadata.json|\n",
      "+-----------------------+------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View metadata file versions\n",
    "metadata_versions = spark.sql(\"SELECT * FROM local.db.sample.metadata_log_entries\")\n",
    "print(\"Metadata versions:\")\n",
    "metadata_versions.select(\"timestamp\", \"file\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Examine Snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|       db|   sample|      false|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES IN local.db\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `timestamp` cannot be resolved. Did you mean one of the following? [`operation`, `parent_id`, `summary`, `committed_at`, `manifest_list`].;\n'Project [snapshot_id#92L, 'timestamp, operation#94, manifest_list#95]\n+- Project [committed_at#91, snapshot_id#92L, parent_id#93L, operation#94, manifest_list#95, summary#96]\n   +- SubqueryAlias local.db.sample.snapshots\n      +- RelationV2[committed_at#91, snapshot_id#92L, parent_id#93L, operation#94, manifest_list#95, summary#96] local.db.sample.snapshots local.db.sample.snapshots\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m snapshots \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT * FROM local.db.sample.snapshots\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Display snapshot details\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43msnapshots\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msnapshot_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtimestamp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moperation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmanifest_list\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow(truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:3229\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   3184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   3185\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   3186\u001b[0m \n\u001b[1;32m   3187\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3227\u001b[0m \u001b[38;5;124;03m    +-----+---+\u001b[39;00m\n\u001b[1;32m   3228\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3229\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `timestamp` cannot be resolved. Did you mean one of the following? [`operation`, `parent_id`, `summary`, `committed_at`, `manifest_list`].;\n'Project [snapshot_id#92L, 'timestamp, operation#94, manifest_list#95]\n+- Project [committed_at#91, snapshot_id#92L, parent_id#93L, operation#94, manifest_list#95, summary#96]\n   +- SubqueryAlias local.db.sample.snapshots\n      +- RelationV2[committed_at#91, snapshot_id#92L, parent_id#93L, operation#94, manifest_list#95, summary#96] local.db.sample.snapshots local.db.sample.snapshots\n"
     ]
    }
   ],
   "source": [
    "# Query Iceberg metadata table: snapshots\n",
    "snapshots = spark.sql(\"SELECT * FROM local.db.sample.snapshots\")\n",
    "\n",
    "# Display snapshot details\n",
    "snapshots.select(\"snapshot_id\", \"timestamp\", \"operation\", \"manifest_list\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Inspect Manifest Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View manifest lists\n",
    "manifests = spark.sql(\"SELECT * FROM local.db.sample.manifests\")\n",
    "print(\"Manifest lists:\")\n",
    "manifests.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Examine Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View all data files\n",
    "data_files = spark.sql(\"SELECT * FROM local.db.sample.files\")\n",
    "print(\"Data files in manifests:\")\n",
    "data_files.select(\"file_path\", \"partition\", \"record_count\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Schema Evolution Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new column\n",
    "spark.sql(\"ALTER TABLE local.db.sample ADD COLUMN new_col string\")\n",
    "\n",
    "# Insert data with new schema (with proper date handling)\n",
    "new_data = [\n",
    "    (4, \"D\", \"2023-02-15\", \"value1\"),\n",
    "    (5, \"E\", \"2023-03-01\", \"value2\")\n",
    "]\n",
    "\n",
    "new_schema = StructType([\n",
    "    StructField(\"id\", LongType()),\n",
    "    StructField(\"data\", StringType()),\n",
    "    StructField(\"dt\", DateType()),\n",
    "    StructField(\"new_col\", StringType())\n",
    "])\n",
    "\n",
    "new_df = spark.createDataFrame(new_data, schema=new_schema)\n",
    "new_df = new_df.withColumn(\"dt\", to_date(col(\"dt\")))\n",
    "new_df.writeTo(\"local.db.sample\").append()\n",
    "\n",
    "# Verify schema change\n",
    "print(\"Updated schema:\")\n",
    "spark.sql(\"DESCRIBE local.db.sample\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Time Travel Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all snapshot IDs\n",
    "snapshot_ids = [row.snapshot_id for row in spark.sql(\"SELECT snapshot_id FROM local.db.sample.snapshots\").collect()]\n",
    "\n",
    "# Query data at each snapshot\n",
    "for snap_id in snapshot_ids:\n",
    "    print(f\"\\nData at snapshot {snap_id}:\")\n",
    "    spark.read \\\n",
    "        .option(\"snapshot-id\", snap_id) \\\n",
    "        .table(\"local.db.sample\") \\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Metadata Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compact data files\n",
    "print(\"Running data file compaction...\")\n",
    "spark.sql(\"CALL local.system.rewrite_data_files(table => 'local.db.sample')\")\n",
    "\n",
    "# Expire old snapshots\n",
    "print(\"\\nExpiring old snapshots...\")\n",
    "spark.sql(\"CALL local.system.expire_snapshots(table => 'local.db.sample', older_than => TIMESTAMP '2023-01-01 00:00:00')\")\n",
    "\n",
    "# View results\n",
    "print(\"\\nFiles after optimization:\")\n",
    "spark.sql(\"SELECT * FROM local.db.sample.files\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the table\n",
    "spark.sql(\"DROP TABLE local.db.sample\")\n",
    "spark.sql(\"DROP DATABASE local.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Date Handling Fixes**:\n",
    "   - Always convert string dates to proper DateType before writing\n",
    "   - Use explicit schemas when creating DataFrames\n",
    "   - Prefer `to_date()` function for conversions\n",
    "\n",
    "2. **Iceberg Metadata Layers**:\n",
    "   - **Metadata Files**: Versioned JSON with table schema and snapshots\n",
    "   - **Manifest Lists**: Index of manifest files with statistics\n",
    "   - **Manifest Files**: List of data files with detailed metrics\n",
    "   - **Data Files**: Actual Parquet/AVRO/ORC files\n",
    "\n",
    "3. **Best Practices**:\n",
    "   - Use DataFrame API with proper schemas for type safety\n",
    "   - Regularly compact files and expire snapshots\n",
    "   - Leverage time travel for data auditing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
