{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Partitioning in PySpark with Apache Iceberg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 38742)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Create a Spark session with Iceberg configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IcebergComplexSchemaEvolution\") \\\n",
    "    .config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.local.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.local.warehouse\", \"/home/jovyan/iceberg/warehouse\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a database if it doesn't exist\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS local.db\")\n",
    "spark.sql(\"USE local.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Hidden Partitioning\n",
    "\n",
    "Hidden partitioning in Iceberg means:\n",
    "1. Partitioning is defined in the table metadata\n",
    "2. You don't need to include partition columns in your queries\n",
    "3. Iceberg automatically prunes partitions based on filter conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Time-Based Hidden Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a table with hidden partitioning by month\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS events (\n",
    "    event_id bigint,\n",
    "    event_time timestamp,\n",
    "    user_id bigint,\n",
    "    event_type string,\n",
    "    details string\n",
    ") USING iceberg\n",
    "PARTITIONED BY (months(event_time))\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some sample data\n",
    "data = []\n",
    "for i in range(1, 1001):\n",
    "    event_time = datetime(2023, 1, 1) + timedelta(days=random.randint(0, 364))\n",
    "    data.append((\n",
    "        i,\n",
    "        event_time,\n",
    "        random.randint(1, 100),\n",
    "        random.choice([\"click\", \"view\", \"purchase\", \"login\"]),\n",
    "        f\"details_{i}\"\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame and write to the table\n",
    "events_df = spark.createDataFrame(data, [\"event_id\", \"event_time\", \"user_id\", \"event_type\", \"details\"])\n",
    "events_df.writeTo(\"local.db.events\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+----------------------+------------------------+\n",
      "|partition|spec_id|record_count|file_count|total_data_file_size_in_bytes|position_delete_record_count|position_delete_file_count|equality_delete_record_count|equality_delete_file_count|last_updated_at       |last_updated_snapshot_id|\n",
      "+---------+-------+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+----------------------+------------------------+\n",
      "|{646}    |0      |80          |1         |2438                         |0                           |0                         |0                           |0                         |2025-07-11 20:05:44.68|3093749453819535185     |\n",
      "|{647}    |0      |92          |1         |2523                         |0                           |0                         |0                           |0                         |2025-07-11 20:05:44.68|3093749453819535185     |\n",
      "|{636}    |0      |90          |1         |2489                         |0                           |0                         |0                           |0                         |2025-07-11 20:05:44.68|3093749453819535185     |\n",
      "|{637}    |0      |78          |1         |2411                         |0                           |0                         |0                           |0                         |2025-07-11 20:05:44.68|3093749453819535185     |\n",
      "|{640}    |0      |84          |1         |2479                         |0                           |0                         |0                           |0                         |2025-07-11 20:05:44.68|3093749453819535185     |\n",
      "|{641}    |0      |85          |1         |2476                         |0                           |0                         |0                           |0                         |2025-07-11 20:05:44.68|3093749453819535185     |\n",
      "|{638}    |0      |91          |1         |2502                         |0                           |0                         |0                           |0                         |2025-07-11 20:05:44.68|3093749453819535185     |\n",
      "|{639}    |0      |75          |1         |2410                         |0                           |0                         |0                           |0                         |2025-07-11 20:05:44.68|3093749453819535185     |\n",
      "|{644}    |0      |76          |1         |2394                         |0                           |0                         |0                           |0                         |2025-07-11 20:05:44.68|3093749453819535185     |\n",
      "|{645}    |0      |90          |1         |2472                         |0                           |0                         |0                           |0                         |2025-07-11 20:05:44.68|3093749453819535185     |\n",
      "|{642}    |0      |82          |1         |2440                         |0                           |0                         |0                           |0                         |2025-07-11 20:05:44.68|3093749453819535185     |\n",
      "|{643}    |0      |77          |1         |2412                         |0                           |0                         |0                           |0                         |2025-07-11 20:05:44.68|3093749453819535185     |\n",
      "+---------+-------+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+----------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the partitions (note the partition values are not actual columns)\n",
    "spark.sql(\"SELECT * FROM local.db.events.partitions\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying with Hidden Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query with a time filter - Iceberg will automatically prune partitions\n",
    "jan_events = spark.sql(\"\"\"\n",
    "SELECT * FROM events\n",
    "WHERE event_time BETWEEN '2023-01-01' AND '2023-01-31'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [*]\n",
      "+- 'Filter (('event_time >= 2023-01-01) AND ('event_time <= 2023-01-31))\n",
      "   +- 'UnresolvedRelation [events], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "event_id: bigint, event_time: timestamp, user_id: bigint, event_type: string, details: string\n",
      "Project [event_id#103L, event_time#104, user_id#105L, event_type#106, details#107]\n",
      "+- Filter ((event_time#104 >= cast(2023-01-01 as timestamp)) AND (event_time#104 <= cast(2023-01-31 as timestamp)))\n",
      "   +- SubqueryAlias local.db.events\n",
      "      +- RelationV2[event_id#103L, event_time#104, user_id#105L, event_type#106, details#107] local.db.events local.db.events\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Filter (event_time#104 <= 2023-01-31 00:00:00)\n",
      "+- RelationV2[event_id#103L, event_time#104, user_id#105L, event_type#106, details#107] local.db.events\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Filter (event_time#104 <= 2023-01-31 00:00:00)\n",
      "+- *(1) ColumnarToRow\n",
      "   +- BatchScan local.db.events[event_id#103L, event_time#104, user_id#105L, event_type#106, details#107] local.db.events (branch=null) [filters=event_time IS NOT NULL, event_time >= 1672531200000000, event_time <= 1675123200000000, groupedBy=] RuntimeFilters: []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explain plan shows partition pruning\n",
    "jan_events.explain(extended=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [*]\n",
      "+- 'UnresolvedRelation [events], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "event_id: bigint, event_time: timestamp, user_id: bigint, event_type: string, details: string\n",
      "Project [event_id#123L, event_time#124, user_id#125L, event_type#126, details#127]\n",
      "+- SubqueryAlias local.db.events\n",
      "   +- RelationV2[event_id#123L, event_time#124, user_id#125L, event_type#126, details#127] local.db.events local.db.events\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "RelationV2[event_id#123L, event_time#124, user_id#125L, event_type#126, details#127] local.db.events\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) ColumnarToRow\n",
      "+- BatchScan local.db.events[event_id#123L, event_time#124, user_id#125L, event_type#126, details#127] local.db.events (branch=null) [filters=, groupedBy=] RuntimeFilters: []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare with a query that doesn't benefit from pruning\n",
    "all_events = spark.sql(\"SELECT * FROM events\")\n",
    "all_events.explain(extended=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Bucket Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a table with bucket partitioning\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS user_sessions (\n",
    "    session_id bigint,\n",
    "    user_id bigint,\n",
    "    start_time timestamp,\n",
    "    end_time timestamp,\n",
    "    pages_visited int\n",
    ") USING iceberg\n",
    "PARTITIONED BY (bucket(16, user_id))\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data\n",
    "sessions_data = []\n",
    "for i in range(1, 1001):\n",
    "    sessions_data.append((\n",
    "        i,\n",
    "        random.randint(1, 1000),\n",
    "        datetime(2023, 1, 1) + timedelta(minutes=random.randint(0, 1440)),\n",
    "        datetime(2023, 1, 1) + timedelta(minutes=random.randint(1440, 2880)),\n",
    "        random.randint(1, 20)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_df = spark.createDataFrame(sessions_data, \n",
    "                                  [\"session_id\", \"user_id\", \"start_time\", \"end_time\", \"pages_visited\"])\n",
    "sessions_df.writeTo(\"local.db.user_sessions\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+--------------------+------------------------+\n",
      "|partition|spec_id|record_count|file_count|total_data_file_size_in_bytes|position_delete_record_count|position_delete_file_count|equality_delete_record_count|equality_delete_file_count|     last_updated_at|last_updated_snapshot_id|\n",
      "+---------+-------+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+--------------------+------------------------+\n",
      "|      {8}|      0|          51|         1|                         2547|                           0|                         0|                           0|                         0|2025-07-11 20:08:...|     8921799163203253987|\n",
      "|      {9}|      0|          75|         1|                         2745|                           0|                         0|                           0|                         0|2025-07-11 20:08:...|     8921799163203253987|\n",
      "|      {6}|      0|          62|         1|                         2710|                           0|                         0|                           0|                         0|2025-07-11 20:08:...|     8921799163203253987|\n",
      "|      {7}|      0|          62|         1|                         2711|                           0|                         0|                           0|                         0|2025-07-11 20:08:...|     8921799163203253987|\n",
      "|     {12}|      0|          54|         1|                         2579|                           0|                         0|                           0|                         0|2025-07-11 20:08:...|     8921799163203253987|\n",
      "|     {13}|      0|          37|         1|                         2349|                           0|                         0|                           0|                         0|2025-07-11 20:08:...|     8921799163203253987|\n",
      "|     {10}|      0|          90|         1|                         2930|                           0|                         0|                           0|                         0|2025-07-11 20:08:...|     8921799163203253987|\n",
      "|     {11}|      0|          67|         1|                         2772|                           0|                         0|                           0|                         0|2025-07-11 20:08:...|     8921799163203253987|\n",
      "|     {14}|      0|          61|         1|                         2680|                           0|                         0|                           0|                         0|2025-07-11 20:08:...|     8921799163203253987|\n",
      "|     {15}|      0|          60|         1|                         2685|                           0|                         0|                           0|                         0|2025-07-11 20:08:...|     8921799163203253987|\n",
      "|      {0}|      0|          61|         1|                         2728|                           0|                         0|                           0|                         0|2025-07-11 20:08:...|     8921799163203253987|\n",
      "|      {1}|      0|          47|         1|                         2486|                           0|                         0|                           0|                         0|2025-07-11 20:08:...|     8921799163203253987|\n",
      "|      {4}|      0|          67|         1|                         2726|                           0|                         0|                           0|                         0|2025-07-11 20:08:...|     8921799163203253987|\n",
      "|      {5}|      0|          49|         1|                         2510|                           0|                         0|                           0|                         0|2025-07-11 20:08:...|     8921799163203253987|\n",
      "|      {2}|      0|          66|         1|                         2749|                           0|                         0|                           0|                         0|2025-07-11 20:08:...|     8921799163203253987|\n",
      "|      {3}|      0|          91|         1|                         2965|                           0|                         0|                           0|                         0|2025-07-11 20:08:...|     8921799163203253987|\n",
      "+---------+-------+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+--------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show bucket partitioning\n",
    "spark.sql(\"SELECT * FROM local.db.user_sessions.partitions\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying Bucket Partitioned Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query for a specific user - will only scan the relevant bucket\n",
    "user_123_sessions = spark.sql(\"\"\"\n",
    "SELECT * FROM user_sessions\n",
    "WHERE user_id = 123\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [*]\n",
      "+- 'Filter ('user_id = 123)\n",
      "   +- 'UnresolvedRelation [user_sessions], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "session_id: bigint, user_id: bigint, start_time: timestamp, end_time: timestamp, pages_visited: int\n",
      "Project [session_id#248L, user_id#249L, start_time#250, end_time#251, pages_visited#252]\n",
      "+- Filter (user_id#249L = cast(123 as bigint))\n",
      "   +- SubqueryAlias local.db.user_sessions\n",
      "      +- RelationV2[session_id#248L, user_id#249L, start_time#250, end_time#251, pages_visited#252] local.db.user_sessions local.db.user_sessions\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Filter (user_id#249L = 123)\n",
      "+- RelationV2[session_id#248L, user_id#249L, start_time#250, end_time#251, pages_visited#252] local.db.user_sessions\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Filter (user_id#249L = 123)\n",
      "+- *(1) ColumnarToRow\n",
      "   +- BatchScan local.db.user_sessions[session_id#248L, user_id#249L, start_time#250, end_time#251, pages_visited#252] local.db.user_sessions (branch=null) [filters=user_id IS NOT NULL, user_id = 123, groupedBy=] RuntimeFilters: []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_123_sessions.explain(extended=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with a query that can't use bucket pruning\n",
    "all_sessions = spark.sql(\"SELECT * FROM user_sessions WHERE pages_visited > 10\")\n",
    "all_sessions.explain(extended=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Truncate Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table with truncate partitioning\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS product_views (\n",
    "    view_id bigint,\n",
    "    product_sku string,\n",
    "    user_id bigint,\n",
    "    view_time timestamp,\n",
    "    duration_seconds int\n",
    ") USING iceberg\n",
    "PARTITIONED BY (truncate(100, product_sku))\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data\n",
    "skus = [f\"SKU-{i:05d}\" for i in range(1, 1001)]\n",
    "views_data = []\n",
    "for i in range(1, 1001):\n",
    "    views_data.append((\n",
    "        i,\n",
    "        random.choice(skus),\n",
    "        random.randint(1, 1000),\n",
    "        datetime(2023, 1, 1) + timedelta(seconds=random.randint(0, 86400)),\n",
    "        random.randint(1, 300)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "views_df = spark.createDataFrame(views_data, \n",
    "                               [\"view_id\", \"product_sku\", \"user_id\", \"view_time\", \"duration_seconds\"])\n",
    "views_df.writeTo(\"local.db.product_views\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show truncate partitions\n",
    "spark.sql(\"SELECT * FROM local.db.product_views.partitions\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying Truncate Partitioned Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query for products in a specific SKU range\n",
    "popular_skus = spark.sql(\"\"\"\n",
    "SELECT * FROM product_views\n",
    "WHERE product_sku LIKE 'SKU-01%'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_skus.explain(extended=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Example: Nested Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table with nested partitioning\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS server_logs (\n",
    "    log_id bigint,\n",
    "    server_id string,\n",
    "    event_time timestamp,\n",
    "    log_level string,\n",
    "    message string\n",
    ") USING iceberg\n",
    "PARTITIONED BY (days(event_time), bucket(8, server_id), log_level)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data\n",
    "servers = [f\"server-{i}\" for i in ['a', 'b', 'c', 'd', 'e']]\n",
    "levels = [\"INFO\", \"WARN\", \"ERROR\", \"DEBUG\"]\n",
    "log_data = []\n",
    "for i in range(1, 1001):\n",
    "    log_data.append((\n",
    "        i,\n",
    "        random.choice(servers),\n",
    "        datetime(2023, 1, 1) + timedelta(hours=random.randint(0, 24*30)),\n",
    "        random.choice(levels),\n",
    "        f\"Log message {i}\"\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_df = spark.createDataFrame(log_data, [\"log_id\", \"server_id\", \"event_time\", \"log_level\", \"message\"])\n",
    "logs_df.writeTo(\"local.db.server_logs\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show nested partitions\n",
    "spark.sql(\"SELECT * FROM local.db.server_logs.partitions\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying Nested Partitioned Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query that benefits from all partition levels\n",
    "filtered_logs = spark.sql(\"\"\"\n",
    "SELECT * FROM server_logs\n",
    "WHERE event_time BETWEEN '2023-01-15' AND '2023-01-16'\n",
    "  AND server_id = 'server-a'\n",
    "  AND log_level = 'ERROR'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_logs.explain(extended=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query that benefits from some partition levels\n",
    "partial_filter_logs = spark.sql(\"\"\"\n",
    "SELECT * FROM server_logs\n",
    "WHERE event_time BETWEEN '2023-01-15' AND '2023-01-16'\n",
    "  AND log_level IN ('ERROR', 'WARN')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_filter_logs.explain(extended=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add a new partition field to our events table\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE events\n",
    "ADD PARTITION FIELD hours(event_time)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the updated partition spec\n",
    "spark.sql(\"SELECT * FROM local.db.events.partitions\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now writes will use both month and hour partitioning\n",
    "new_events_data = []\n",
    "for i in range(1001, 1101):\n",
    "    event_time = datetime(2023, 6, 15) + timedelta(hours=random.randint(0, 23))\n",
    "    new_events_data.append((\n",
    "        i,\n",
    "        event_time,\n",
    "        random.randint(1, 100),\n",
    "        random.choice([\"click\", \"view\", \"purchase\", \"login\"]),\n",
    "        f\"details_{i}\"\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_events_df = spark.createDataFrame(new_events_data, \n",
    "                                    [\"event_id\", \"event_time\", \"user_id\", \"event_type\", \"details\"])\n",
    "new_events_df.writeTo(\"local.db.events\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the new partitions\n",
    "spark.sql(\"SELECT * FROM local.db.events.partitions\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With partition pruning\n",
    "start_time = time.time()\n",
    "pruned_query = spark.sql(\"\"\"\n",
    "SELECT COUNT(*) FROM events\n",
    "WHERE event_time BETWEEN '2023-06-01' AND '2023-06-30'\n",
    "\"\"\")\n",
    "pruned_query.show()\n",
    "print(f\"Time with partition pruning: {time.time() - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without partition pruning\n",
    "start_time = time.time()\n",
    "full_scan = spark.sql(\"\"\"\n",
    "SELECT COUNT(*) FROM events\n",
    "WHERE event_type = 'click'\n",
    "\"\"\")\n",
    "full_scan.show()\n",
    "print(f\"Time without partition pruning: {time.time() - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check partition sizes\n",
    "spark.sql(\"\"\"\n",
    "SELECT partition, record_count, file_count\n",
    "FROM local.db.events.partitions\n",
    "ORDER BY record_count DESC\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop tables if needed\n",
    "spark.sql(\"DROP TABLE IF EXISTS local.db.events\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS local.db.user_sessions\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS local.db.product_views\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS local.db.server_logs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
