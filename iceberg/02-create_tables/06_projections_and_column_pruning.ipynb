{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cddc6a51",
   "metadata": {},
   "source": [
    "# Projections and Column Pruning in Apache Iceberg with Spark\n",
    "\n",
    "This notebook demonstrates how to use column projection and pruning with Apache Iceberg tables in Spark SQL.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Import Required Functions\n",
    "\n",
    "We start by importing necessary functions from PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4e27be78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59daeeee",
   "metadata": {},
   "source": [
    "## 2. Create the Iceberg Table\n",
    "\n",
    "We drop the table if it exists, then create a new Iceberg table named `sales_projection` with a sample schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "556d2b6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop table if exists\n",
    "spark.sql(\"DROP TABLE IF EXISTS local.db.sales_projection\")\n",
    "\n",
    "# Create table\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE local.db.sales_projection (\n",
    "        id INT,\n",
    "        region STRING,\n",
    "        product STRING,\n",
    "        category STRING,\n",
    "        sales DOUBLE,\n",
    "        discount DOUBLE,\n",
    "        quantity INT,\n",
    "        sales_date DATE\n",
    "    )\n",
    "    USING iceberg\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d22ac7",
   "metadata": {},
   "source": [
    "## 3. Insert Sample Data\n",
    "\n",
    "We create a DataFrame with sample sales data and write it to the Iceberg table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17613292",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (1, 'North', 'Phone', 'Electronics', 500.0, 10.0, 1, '2024-01-01'),\n",
    "    (2, 'South', 'Tablet', 'Electronics', 700.0, 20.0, 2, '2024-01-02'),\n",
    "    (3, 'East', 'Laptop', 'Electronics', 1000.0, 50.0, 1, '2024-01-03'),\n",
    "    (4, 'West', 'Headphones', 'Accessories', 200.0, 5.0, 3, '2024-01-04'),\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"region\", \"product\", \"category\", \"sales\", \"discount\", \"quantity\", \"sales_date\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns) \\\n",
    "    .withColumn(\"sales_date\", to_date(col(\"sales_date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "df.writeTo(\"local.db.sales_projection\").append()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d434c78",
   "metadata": {},
   "source": [
    "## 4. Select All Columns\n",
    "\n",
    "We query the table to display all columns, showing the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "73b6a02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+-----------+------+--------+--------+----------+\n",
      "| id|region|   product|   category| sales|discount|quantity|sales_date|\n",
      "+---+------+----------+-----------+------+--------+--------+----------+\n",
      "|  1| North|     Phone|Electronics| 500.0|    10.0|       1|2024-01-01|\n",
      "|  2| South|    Tablet|Electronics| 700.0|    20.0|       2|2024-01-02|\n",
      "|  3|  East|    Laptop|Electronics|1000.0|    50.0|       1|2024-01-03|\n",
      "|  4|  West|Headphones|Accessories| 200.0|     5.0|       3|2024-01-04|\n",
      "+---+------+----------+-----------+------+--------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select all columns\n",
    "df_full = spark.sql(\"SELECT * FROM local.db.sales_projection\")\n",
    "df_full.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146cec20",
   "metadata": {},
   "source": [
    "## 5. Select Only Required Columns (Projection)\n",
    "\n",
    "We select only the `region` and `sales` columns to demonstrate column projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3ba9caa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|region| sales|\n",
      "+------+------+\n",
      "| North| 500.0|\n",
      "| South| 700.0|\n",
      "|  East|1000.0|\n",
      "|  West| 200.0|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select only region and sales\n",
    "df_proj = spark.sql(\"SELECT region, sales FROM local.db.sales_projection\")\n",
    "df_proj.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14680938",
   "metadata": {},
   "source": [
    "## 6. View the Execution Plan for Projection\n",
    "\n",
    "We use the `EXPLAIN` statement to show the physical execution plan, confirming that only the required columns are read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "84cba3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|plan                                                                                                                                                                                          |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|== Physical Plan ==\\n*(1) ColumnarToRow\\n+- BatchScan local.db.sales_projection[region#1656, sales#1659] local.db.sales_projection (branch=null) [filters=, groupedBy=] RuntimeFilters: []\\n\\n|\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print execution plan to confirm projection\n",
    "spark.sql(\"EXPLAIN SELECT region, sales FROM local.db.sales_projection\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe56422",
   "metadata": {},
   "source": [
    "## 7. Column Projection with Filter Pushdown\n",
    "\n",
    "We select `region` and `sales` for rows where `category` is 'Electronics', demonstrating both projection and filter pushdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7e5c367b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|region| sales|\n",
      "+------+------+\n",
      "| North| 500.0|\n",
      "| South| 700.0|\n",
      "|  East|1000.0|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Column projection + filter pushdown\n",
    "spark.sql(\"\"\"\n",
    "    SELECT region, sales\n",
    "    FROM local.db.sales_projection\n",
    "    WHERE category = 'Electronics'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e881bf",
   "metadata": {},
   "source": [
    "## 8. Execution Plan: Projection + Filter Pushdown\n",
    "\n",
    "We use `EXPLAIN` again to confirm that both column pruning and filter pushdown are applied in the query plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e4780b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|plan                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|== Physical Plan ==\\n*(1) Project [region#1708, sales#1711]\\n+- *(1) Filter (isnotnull(category#1710) AND (category#1710 = Electronics))\\n   +- *(1) ColumnarToRow\\n      +- BatchScan local.db.sales_projection[region#1708, category#1710, sales#1711] local.db.sales_projection (branch=null) [filters=category IS NOT NULL, category = 'Electronics', groupedBy=] RuntimeFilters: []\\n\\n|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    EXPLAIN SELECT region, sales\n",
    "    FROM local.db.sales_projection\n",
    "    WHERE category = 'Electronics'\n",
    "\"\"\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
