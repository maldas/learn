{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d00ae757",
   "metadata": {},
   "source": [
    "# üßä Apache Iceberg + PySpark: Local Hadoop Catalog Setup\n",
    "\n",
    "This notebook demonstrates how to configure a SparkSession to use **Apache Iceberg** with a **local Hadoop catalog**. This setup is ideal for local development and testing without requiring a Hive Metastore.\n",
    "\n",
    "## üîß Spark Configuration Details\n",
    "\n",
    "The following Spark configurations are used to enable Iceberg support:\n",
    "\n",
    "| Config Key | Description |\n",
    "|------------|-------------|\n",
    "| `spark.sql.catalog.local` | Registers a catalog named `local` using Iceberg's `SparkCatalog` implementation. |\n",
    "| `spark.sql.catalog.local.type` | Specifies the catalog type as `hadoop`, meaning metadata is stored in the local filesystem. |\n",
    "| `spark.sql.catalog.local.warehouse` | Sets the warehouse directory path where Iceberg tables will be stored. This should be a valid local or distributed filesystem path. |\n",
    "\n",
    "Once configured, you can use SQL commands to create, insert into, and query Iceberg tables using the `local` catalog.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237a7aa8",
   "metadata": {},
   "source": [
    "# üßä PySpark + Apache Iceberg: Local Hadoop Catalog Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd51b22b",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to configure a SparkSession with Apache Iceberg using a local Hadoop catalog. It also includes example SQL commands to create and query Iceberg tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ead92ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c56fd2",
   "metadata": {},
   "source": [
    "### Define the warehouse path (adjust as needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbb00a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Ensure the warehouse path exists\n",
    "warehouse_path = \"/home/jovyan/iceberg/warehouse\"\n",
    "os.makedirs(warehouse_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf03fe3",
   "metadata": {},
   "source": [
    "## üîß SparkSession Configuration\n",
    "We configure the SparkSession to use Iceberg's `SparkCatalog` with a local Hadoop catalog. Make sure to define a valid `warehouse_path` where Iceberg tables will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0773ebd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SparkSession initialized with local Iceberg Hadoop catalog\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize SparkSession with Iceberg Hadoop catalog\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IcebergLocalSetup\") \\\n",
    "    .config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.local.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.local.warehouse\", warehouse_path) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(\"‚úÖ SparkSession initialized with local Iceberg Hadoop catalog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ce229b",
   "metadata": {},
   "source": [
    "### Optional: List available catalogs and configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c6bd119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.sql.catalog.local.warehouse = /home/jovyan/iceberg/warehouse\n",
      "spark.sql.catalog.local.type = hadoop\n",
      "spark.sql.catalog.local = org.apache.iceberg.spark.SparkCatalog\n"
     ]
    }
   ],
   "source": [
    "for k, v in spark.sparkContext.getConf().getAll():\n",
    "    if 'catalog' in k:\n",
    "        print(f\"{k} = {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1540ab",
   "metadata": {},
   "source": [
    "## üìù Notes\n",
    "- Ensure the Iceberg and Hadoop dependencies are available in your Spark environment.\n",
    "- The `warehouse_path` should be accessible and writable.\n",
    "- This setup is ideal for local development and testing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
