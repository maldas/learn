{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbb941e2",
   "metadata": {},
   "source": [
    "# Change Data Capture (CDC) with Apache Hudi and PySpark\n",
    "This notebook demonstrates how to perform Change Data Capture (CDC) using Apache Hudi with PySpark. We will create a Hudi table, insert initial data, perform updates, and query incremental changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05810807",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SimpleHudiCreate\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.spark.sql.hudi.HoodieSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.hudi.catalog.HoodieCatalog\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8631117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "base_path = \"/home/jovyan/hudi\"\n",
    "table_name = \"hudi_cdc_table\"\n",
    "table_path = os.path.join(base_path, table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64231bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Initial data\n",
    "data = [Row(id=1, name=\"Alice\", ts=1000),\n",
    "        Row(id=2, name=\"Bob\", ts=1000),\n",
    "        Row(id=3, name=\"Charlie\", ts=1000)]\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "# Write initial data to Hudi\n",
    "df.write.format(\"hudi\") \\\n",
    "    .option(\"hoodie.table.name\", table_name) \\\n",
    "    .option(\"hoodie.datasource.write.recordkey.field\", \"id\") \\\n",
    "    .option(\"hoodie.datasource.write.precombine.field\", \"ts\") \\\n",
    "    .option(\"hoodie.datasource.write.operation\", \"insert\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf5053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated data\n",
    "update_data = [Row(id=2, name=\"Bob Updated\", ts=2000),\n",
    "               Row(id=4, name=\"Daisy\", ts=2000)]\n",
    "\n",
    "update_df = spark.createDataFrame(update_data)\n",
    "\n",
    "# Write updates to Hudi\n",
    "update_df.write.format(\"hudi\") \\\n",
    "    .option(\"hoodie.table.name\", table_name) \\\n",
    "    .option(\"hoodie.datasource.write.recordkey.field\", \"id\") \\\n",
    "    .option(\"hoodie.datasource.write.precombine.field\", \"ts\") \\\n",
    "    .option(\"hoodie.datasource.write.operation\", \"upsert\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save(table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55a51b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+-----------+----+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name| id|       name|  ts|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+-----------+----+\n",
      "|  20250714112815243|20250714112815243...|                 1|                      |a25a07da-58a5-419...|  1|      Alice|1000|\n",
      "|  20250714112900617|20250714112900617...|                 2|                      |a25a07da-58a5-419...|  2|Bob Updated|2000|\n",
      "|  20250714112815243|20250714112815243...|                 3|                      |a25a07da-58a5-419...|  3|    Charlie|1000|\n",
      "|  20250714112900617|20250714112900617...|                 4|                      |a25a07da-58a5-419...|  4|      Daisy|2000|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+---+-----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the latest commit time\n",
    "commits = spark.read.format(\"hudi\").load(table_path).select(\"_hoodie_commit_time\").distinct().orderBy(\"_hoodie_commit_time\", ascending=False)\n",
    "\n",
    "commit_times = [row['_hoodie_commit_time'] for row in commits.collect()]\n",
    "begin_time = commit_times[1]  # second latest commit\n",
    "\n",
    "# Read incrementally\n",
    "incremental_df = spark.read.format(\"hudi\") \\\n",
    "    .option(\"hoodie.datasource.query.type\", \"incremental\") \\\n",
    "    .option(\"hoodie.datasource.read.begin.instanttime\", begin_time) \\\n",
    "    .load(table_path)\n",
    "\n",
    "incremental_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b70db7-f9fa-4f3e-89ff-bb51b591704f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
