{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delta Lake Time Travel and Data Versioning\n",
    "\n",
    "Delta Lake is an open-source storage layer that enhances Apache Spark with ACID transactions, enabling reliable data management. Its time travel feature allows you to query previous versions of data, making it ideal for auditing, error recovery, and experiment reproducibility. This notebook demonstrates time travel and data versioning through simple, intermediate, and advanced scenarios using a sample employee dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Environment\n",
    "\n",
    "Ensure you have PySpark and Delta Lake installed. You can install Delta Lake using:\n",
    "\n",
    "```\n",
    "pip install delta-spark\n",
    "```\n",
    "\n",
    "The following code sets up a SparkSession for Delta Lake operations. Adjust configurations based on your environment (e.g., Databricks may not require explicit Delta Lake extensions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Delta Lake Time Travel\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: If running in Databricks, Delta Lake is included by default. For local setups, ensure compatibility with Spark and Delta Lake versions (e.g., Delta Lake 4.0.0 with compatible Spark versions). See the [Delta Lake Quickstart](https://docs.delta.io/latest/quick-start.html) for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Scenario: Creating a Delta Table and Using Time Travel\n",
    "\n",
    "In this section, we’ll create a Delta table for employee data, insert additional records, and use time travel to query previous versions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create Initial Data and Write to Delta Table\n",
    "\n",
    "We define a schema for an employee table and insert three employee records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"employee_id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"department\", StringType(), False),\n",
    "    StructField(\"salary\", DoubleType(), False)\n",
    "])\n",
    "\n",
    "# Initial data\n",
    "initial_data = [\n",
    "    (1, \"Alice\", \"Engineering\", 100000.0),\n",
    "    (2, \"Bob\", \"Sales\", 80000.0),\n",
    "    (3, \"Charlie\", \"Marketing\", 90000.0)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(initial_data, schema)\n",
    "\n",
    "# Write to Delta table\n",
    "df.write.format(\"delta\").save(\"./delta-employees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: View Current Data\n",
    "\n",
    "Let’s display the current state of the Delta table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and display the Delta table\n",
    "spark.read.format(\"delta\").load(\"./delta-employees\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "\n",
    "```\n",
    "+-----------+-------+------------+--------+\n",
    "|employee_id|   name|  department|  salary|\n",
    "+-----------+-------+------------+--------+\n",
    "|          1|  Alice| Engineering|100000.0|\n",
    "|          2|    Bob|       Sales| 80000.0|\n",
    "|          3|Charlie|   Marketing| 90000.0|\n",
    "+-----------+-------+------------+--------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Insert Additional Data\n",
    "\n",
    "We append two more employees to the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New data\n",
    "new_data = [\n",
    "    (4, \"David\", \"Engineering\", 95000.0),\n",
    "    (5, \"Eve\", \"Sales\", 85000.0)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df_new = spark.createDataFrame(new_data, schema)\n",
    "\n",
    "# Append to Delta table\n",
    "df_new.write.format(\"delta\").mode(\"append\").save(\"./delta-employees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: View Updated Data\n",
    "\n",
    "Display milestones: Display the table after appending new records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and display the updated Delta table\n",
    "spark.read.format(\"delta\").load(\"./delta-employees\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "\n",
    "```\n",
    "+-----------+-------+------------+--------+\n",
    "|employee_id|   name|  department|  salary|\n",
    "+-----------+-------+------------+--------+\n",
    "|          1|  Alice| Engineering|100000.0|\n",
    "|          2|    Bob|       Sales| 80000.0|\n",
    "|          3|Charlie|   Marketing| 90000.0|\n",
    "|          4|  David| Engineering| 95000.0|\n",
    "|          5|    Eve|       Sales| 85000.0|\n",
    "+-----------+-------+------------+--------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Use Time Travel to Query Previous Versions\n",
    "\n",
    "Delta Lake’s transaction log tracks all changes, allowing us to query earlier versions. First, we check the table’s history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe history\n",
    "spark.sql(\"DESCRIBE HISTORY './delta-employees'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output (example, timestamps will vary):**\n",
    "\n",
    "| version | timestamp           | operation | ... |\n",
    "|---------|---------------------|-----------|-----|\n",
    "| 1       | 2025-07-14 23:59:00 | WRITE     | ... |\n",
    "| 0       | 2025-07-14 23:58:00 | WRITE     | ... |\n",
    "\n",
    "Now, query version 0 (initial state with three employees)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read version 0\n",
    "spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"./delta-employees\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "\n",
    "```\n",
    "+-----------+-------+------------+--------+\n",
    "|employee_id|   name|  department|  salary|\n",
    "+-----------+-------+------------+--------+\n",
    "|          1|  Alice| Engineering|100000.0|\n",
    "|          2|    Bob|       Sales| 80000.0|\n",
    "|          3|Charlie|   Marketing| 90000.0|\n",
    "+-----------+-------+------------+--------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read version 1\n",
    "spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(\"./delta-employees\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "\n",
    "```\n",
    "+-----------+-------+------------+--------+\n",
    "|employee_id|   name|  department|  salary|\n",
    "+-----------+-------+------------+--------+\n",
    "|          1|  Alice| Engineering|100000.0|\n",
    "|          2|    Bob|       Sales| 80000.0|\n",
    "|          3|Charlie|   Marketing| 90000.0|\n",
    "|          4|  David| Engineering| 95000.0|\n",
    "|          5|    Eve|       Sales| 85000.0|\n",
    "+-----------+-------+------------+--------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   " кожу": {},
   "source": [
    "## Intermediate Scenario: Updates, Deletes, and Time Travel\n",
    "\n",
    "This section demonstrates updating and deleting records, then using time travel to inspect the table’s state at different points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Update Records\n",
    "\n",
    "We update Alice’s salary to 110,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Load Delta table\n",
    "delta_table = DeltaTable.forPath(spark, \"./delta-employees\")\n",
    "\n",
    "# Update Alice's salary\n",
    "delta_table.update(\"employee_id = 1\", {\"salary\": \"110000.0\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Delete Records\n",
    "\n",
    "We delete Bob from the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete Bob\n",
    "delta_table.delete(\"employee_id = 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: View Current Data\n",
    "\n",
    "Display the table after updates and deletes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and display the current Delta table\n",
    "spark.read.format(\"delta\").load(\"./delta-employees\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "\n",
    "```\n",
    "+-----------+-------+------------+--------+\n",
    "|employee_id|   name|  department|  salary|\n",
    "+-----------+-------+------------+--------+\n",
    "|          1|  Alice| Engineering|110000.0|\n",
    "|          3|Charlie|   Marketing| 90000.0|\n",
    "|          4|  David| Engineering| 95000.0|\n",
    "|          5|    Eve|       Sales| 85000.0|\n",
    "+-----------+-------+------------+--------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Use Time Travel to View Previous States\n",
    "\n",
    "Check the table’s history to see all operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe history\n",
    "spark.sql(\"DESCRIBE HISTORY './delta-employees'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output (example, timestamps will vary):**\n",
    "\n",
    "| version | timestamp           | operation | ... |\n",
    "|---------|---------------------|-----------|-----|\n",
    "| 3       | 2025-07-14 23:59:30 | DELETE    | ... |\n",
    "| 2       | 2025-07-14 23:59:15 | UPDATE    | ... |\n",
    "| 1       | 2025-07-14 23:59:00 | WRITE     | ... |\n",
    "| 0       | 2025-07-14 23:58:00 | WRITE     | ... |\n",
    "\n",
    "Query version 1 (before updates and deletes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read version 1\n",
    "spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(\"./delta-employees\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "\n",
    "```\n",
    "+-----------+-------+------------+--------+\n",
    "|employee_id|   name|  department|  salary|\n",
    "+-----------+-------+------------+--------+\n",
    "|          1|  Alice| Engineering|100000.0|\n",
    "|          2|    Bob|       Sales| 80000.0|\n",
    "|          3|Charlie|   Marketing| 90000.0|\n",
    "|          4|  David| Engineering| 95000.0|\n",
    "|          5|    Eve|       Sales| 85000.0|\n",
    "+-----------+-------+------------+--------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read version 2\n",
    "spark.read.format(\"delta\").option(\"versionAsOf\", 2).load(\"./delta-employees\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "\n",
    "```\n",
    "+-----------+-------+------------+--------+\n",
    "|employee_id|   name|  department|  salary|\n",
    "+-----------+-------+------------+--------+\n",
    "|          1|  Alice| Engineering|110000.0|\n",
    "|          2|    Bob|       Sales| 80000.0|\n",
    "|          3|Charlie|   Marketing| 90000.0|\n",
    "|          4|  David| Engineering| 95000.0|\n",
    "|          5|    Eve|       Sales| 85000.0|\n",
    "+-----------+-------+------------+--------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Scenario: Merge Operations, Schema Evolution, and Restoring Versions\n",
    "\n",
    "This section covers complex operations like merging data, evolving the schema, using time travel with timestamps, and restoring a previous version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Perform a Merge Operation\n",
    "\n",
    "We merge a dataset that updates Alice’s salary and adds a new employee, Frank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data to merge\n",
    "merge_data = [\n",
    "    (1, \"Alice\", \"Engineering\", 120000.0),  # Update Alice's salary\n",
    "    (6, \"Frank\", \"HR\", 70000.0)           # New employee\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df_merge = spark.createDataFrame(merge_data, schema)\n",
    "\n",
    "# Merge into Delta table\n",
    "delta_table.alias(\"target\").merge(\n",
    "    df_merge.alias(\"source\"),\n",
    "    \"target.employee_id = source.employee_id\"\n",
    ").whenMatchedUpdate(set={\n",
    "    \"name\": \"source.name\",\n",
    "    \"department\": \"source.department\",\n",
    "    \"salary\": \"source.salary\"\n",
    "}).whenNotMatchedInsert(values={\n",
    "    \"employee_id\": \"source.employee_id\",\n",
    "    \"name\": \"source.name\",\n",
    "    \"department\": \"source.department\",\n",
    "    \"salary\": \"source.salary\"\n",
    "}).execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: View Current Data After Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and display the Delta table\n",
    "spark.read.format(\"delta\").load(\"./delta-employees\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "\n",
    "```\n",
    "+-----------+-------+------------+--------+\n",
    "|employee_id|   name|  department|  salary|\n",
    "+-----------+-------+------------+--------+\n",
    "|          1|  Alice| Engineering|120000.0|\n",
    "|          3|Charlie|   Marketing| 90000.0|\n",
    "|          4|  David| Engineering| 95000.0|\n",
    "|          5|    Eve|       Sales| 85000.0|\n",
    "|          6|  Frank|          HR| 70000.0|\n",
    "+-----------+-------+------------+--------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Evolve Schema by Adding a New Column\n",
    "\n",
    "We add a `hire_date` column by appending a new employee with this field, enabling schema evolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DateType\n",
    "import datetime\n",
    "\n",
    "# New employee with hire_date\n",
    "new_employee = [\n",
    "    (7, \"Grace\", \"Finance\", 100000.0, datetime.date(2025, 1, 1))\n",
    "]\n",
    "\n",
    "# Updated schema with hire_date\n",
    "schema_with_date = schema.add(\"hire_date\", DateType())\n",
    "\n",
    "# Create DataFrame\n",
    "df_new_with_date = spark.createDataFrame(new_employee, schema_with_date)\n",
    "\n",
    "# Write with schema evolution\n",
    "df_new_with_date.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").save(\"./delta-employees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: View Current Data with New Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and display the Delta table\n",
    "spark.read.format(\"delta\").load(\"./delta-employees\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "\n",
    "```\n",
    "+-----------+-------+------------+--------+----------+\n",
    "|employee_id|   name|  department|  salary| hire_date|\n",
    "+-----------+-------+------------+--------+----------+\n",
    "|          1|  Alice| Engineering|120000.0|      null|\n",
    "|          3|Charlie|   Marketing| 90000.0|      null|\n",
    "|          4|  David| Engineering| 95000.0|      null|\n",
    "|          5|    Eve|       Sales| 85000.0|      null|\n",
    "|          6|  Frank|          HR| 70000.0|      null|\n",
    "|          7|  Grace|     Finance|100000.0|2025-01-01|\n",
    "+-----------+-------+------------+--------+----------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Use Time Travel with Timestamps\n",
    "\n",
    "We use timestamps to query a previous version of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe history\n",
    "history = spark.sql(\"DESCRIBE HISTORY './delta-employees'\")\n",
    "history.select(\"version\", \"timestamp\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output (example, timestamps will vary):**\n",
    "\n",
    "| version | timestamp           |\n",
    "|---------|---------------------|\n",
    "| 4       | 2025-07-14 23:59:45 |\n",
    "| 3       | 2025-07-14 23:59:30 |\n",
    "| 2       | 2025-07-14 23:59:15 |\n",
    "| 1       | 2025-07-14 23:59:00 |\n",
    "| 0       | 2025-07-14 23:58:00 |\n",
    "\n",
    "Select the timestamp for version 1 and query the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get timestamp of version 1\n",
    "version1_timestamp = history.filter(\"version = 1\").select(\"timestamp\").collect()[0][0]\n",
    "\n",
    "# Read using timestamp\n",
    "spark.read.format(\"delta\").option(\"timestampAsOf\", version1_timestamp).load(\"./delta-employees\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "\n",
    "```\n",
    "+-----------+-------+------------+--------+\n",
    "|employee_id|   name|  department|  salary|\n",
    "+-----------+-------+------------+--------+\n",
    "|          1|  Alice| Engineering|100000.0|\n",
    "|          2|    Bob|       Sales| 80000.0|\n",
    "|          3|Charlie|   Marketing| 90000.0|\n",
    "|          4|  David| Engineering| 95000.0|\n",
    "|          5|    Eve|       Sales| 85000.0|\n",
    "+-----------+-------+------------+--------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Restore to a Previous Version\n",
    "\n",
    "We restore the table to version 1, reverting all changes made after the initial append."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore to version 1\n",
    "delta_table.restoreToVersion(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: View Data After Restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and display the Delta table\n",
    "spark.read.format(\"delta\").load(\"./delta-employees\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "\n",
    "```\n",
    "+-----------+-------+------------+--------+\n",
    "|employee_id|   name|  department|  salary|\n",
    "+-----------+-------+------------+--------+\n",
    "|          1|  Alice| Engineering|100000.0|\n",
    "|          2|    Bob|       Sales| 80000.0|\n",
    "|          3|Charlie|   Marketing| 90000.0|\n",
    "|          4|  David| Engineering| 95000.0|\n",
    "|          5|    Eve|       Sales| 85000.0|\n",
    "+-----------+-------+------------+--------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Restoring creates a new version in the transaction log, which you can verify using `DESCRIBE HISTORY`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated Delta Lake’s time travel and data versioning capabilities. You learned how to:\n",
    "- Create and modify Delta tables.\n",
    "- Use time travel to query previous versions by version number or timestamp.\n",
    "- Perform updates, deletes, merges, and schema evolution.\n",
    "- Restore a table to a previous state.\n",
    "\n",
    "These features make Delta Lake a robust solution for managing big data with reliability and flexibility. For more details, explore the [Delta Lake Documentation](https://docs.delta.io/latest/index.html) and [Databricks Time Travel Blog](https://www.databricks.com/blog/2019/02/04/introducing-delta-time-travel-for-large-scale-data-lakes.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
