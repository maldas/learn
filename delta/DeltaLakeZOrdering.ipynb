{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Delta Lake Z-Ordering\n",
        "\n",
        "Welcome to this Jupyter Notebook on Delta Lake Z-Ordering. In this notebook, you will learn how to use Z-Ordering to optimize the performance of your Delta tables. Z-Ordering is a technique that helps in co-locating related data together, which can significantly improve query performance, especially for large datasets.\n",
        "\n",
        "We will cover three scenarios:\n",
        "\n",
        "1. **Simple Z-Ordering**: Applying Z-Ordering on a single column.\n",
        "2. **Intermediate Z-Ordering**: Applying Z-Ordering on multiple columns.\n",
        "3. **Advanced Z-Ordering**: Combining Z-Ordering with partitioning and handling skewed data.\n",
        "\n",
        "Each section includes code examples, explanations, and expected outputs to help you understand the concepts and see the benefits of Z-Ordering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setting up the Environment\n",
        "\n",
        "Before we begin, make sure you have PySpark and Delta Lake installed. You can install them using pip:\n",
        "\n",
        "```bash\n",
        "pip install pyspark delta-spark\n",
        "```\n",
        "\n",
        "Then, configure your Spark session to use Delta Lake:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Delta Lake Z-Ordering\") \\\n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simple Z-Ordering Example\n",
        "\n",
        "In this section, we will create a Delta table with sales data and apply Z-Ordering on the 'date' column. We will then query the data to see how Z-Ordering improves performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Generate Sample Data\n",
        "\n",
        "We will generate a sample sales dataset with columns: `id`, `date`, `product_id`, and `sales_amount`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import expr\n",
        "\n",
        "# Generate sample data\n",
        "df = spark.range(0, 1000000).selectExpr(\n",
        "    \"cast(id as string) as id\",\n",
        "    \"date_add('2023-01-01', cast(rand() * 30 as int)) as date\",\n",
        "    \"concat('P', lpad(cast(cast(rand() * 100 as int) as string), 3, '0')) as product_id\",\n",
        "    \"cast(rand() * 1000 as double) as sales_amount\"\n",
        ")\n",
        "\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Sample Output:**\n",
        "```\n",
        "+---+----------+----------+------------+\n",
        "| id|      date|product_id|sales_amount|\n",
        "+---+----------+----------+------------+\n",
        "|  0|2023-01-15|      P042|   123.45678|\n",
        "|  1|2023-01-22|      P087|   987.65432|\n",
        "|  2|2023-01-07|      P015|   456.78901|\n",
        "|  3|2023-01-29|      P099|   234.56789|\n",
        "|  4|2023-01-12|      P063|   678.90123|\n",
        "+---+----------+----------+------------+\n",
        "```

      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Write Data to Delta Table\n",
        "\n",
        "We will write the data to a Delta table in append mode multiple times to create multiple files, simulating a less optimized layout."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(10):\n",
        "    df_sample = df.sample(fraction=0.1)\n",
        "    df_sample.write.format(\"delta\").mode Debian:mode(\"append\").save(\"/tmp/sales_delta\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Query Before Z-Ordering\n",
        "\n",
        "Let's query the data for a specific date and examine the query plan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = spark.read.format(\"delta\").load(\"/tmp/sales_delta\").filter(\"date = '2023-01-15'\").select(\"sales_amount\")\n",
        "query.explain()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Expected Output:**\n",
        "The query plan will indicate that Spark needs to scan multiple files because the data is not yet optimized. Look for a section in the plan showing a large number of files being read."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Apply Z-Ordering\n",
        "\n",
        "Now, we will apply Z-Ordering on the `date` column to optimize the data layout."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from delta.tables import DeltaTable\n",
        "\n",
        "delta_table = DeltaTable.forPath(spark, \"/tmp/sales_delta\")\n",
        "delta_table.optimize().executeZOrderBy(\"date\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5: Query After Z-Ordering\n",
        "\n",
        "Run the same query again and check the query plan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = spark.read.format(\"delta\").load(\"/tmp/sales_delta\").filter(\"date = '2023-01-15'\").select(\"sales_amount\")\n",
        "query.explain()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Expected Output:**\n",
        "The query plan should now show that fewer files are scanned, as Z-Ordering has co-located data by `date`, allowing file pruning based on min-max statistics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Intermediate Z-Ordering Example\n",
        "\n",
        "In this section, we will work with a customer dataset and apply Z-Ordering on multiple columns: `age` and `location`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Generate Sample Data\n",
        "\n",
        "We will create a sample customer dataset with columns: `customer_id`, `age`, `location`, and `purchase_history`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate sample data\n",
        "df_customers = spark.range(0, 1000000).selectExpr(\n",
        "    \"cast(id as string) as customer_id\",\n",
        "    \"cast(rand() * 47 + 18 as int) as age\",\n",
        "    \"array('New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix')[cast(rand() * 5 as int)] as location\",\n",
        "    \"cast(rand() * 10000 as double) as purchase_history\"\n",
        ")\n",
        "\n",
        "df_customers.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Sample Output:**\n",
        "```\n",
        "+-----------+---+-----------+----------------+\n",
        "|customer_id|age|   location|purchase_history|\n",
        "+-----------+---+-----------+----------------+\n",
        "|          0| 35|   New York|       1234.5678|\n",
        "|          1| 42|Los Angeles|       8765.4321|\n",
        "|          2| 23|    Chicago|       3456.7890|\n",
        "|          3| 58|    Houston|       6543.2109|\n",
        "|          4| 29|    Phoenix|       4321.0987|\n",
        "+-----------+---+-----------+----------------+\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Write Data to Delta Table\n",
        "\n",
        "Write the data in append mode to create multiple files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(10):\n",
        "    df_sample = df_customers.sample(fraction=0.1)\n",
        "    df_sample.write.format(\"delta\").mode(\"append\").save(\"/tmp/customers_delta\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Query Before Z-Ordering\n",
        "\n",
        "Query the data with filters on `age` and `location`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = spark.read.format(\"delta\").load(\"/tmp/customers_delta\").filter(\"age = 30 and location = 'New York'\").select(\"purchase_history\")\n",
        "query.explain()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Expected Output:**\n",
        "The plan will show that many files are scanned due to the unoptimized layout."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Apply Z-Ordering on Multiple Columns\n",
        "\n",
        "Apply Z-Ordering on `age` and `location`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "delta_table_customers = DeltaTable.forPath(spark, \"/tmp/customers_delta\")\n",
        "delta_table_customers.optimize().executeZOrderBy(\"age\", \"location\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5: Query After Z-Ordering\n",
        "\n",
        "Run the same query again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = spark.read.format(\"delta\").load(\"/tmp/customers_delta\").filter(\"age = 30 and location = 'New York'\").select(\"purchase_history\")\n",
        "query.explain()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Expected Output:**\n",
        "The query plan should indicate fewer files are read, demonstrating the optimization from Z-Ordering on multiple columns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced Z-Ordering Scenarios\n",
        "\n",
        "In this section, we will explore complex use cases: combining Z-Ordering with partitioning and handling skewed data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Scenario 1: Combining Partitioning and Z-Ordering\n",
        "\n",
        "We will create a time-series dataset, partition it by `year`, and apply Z-Ordering on `date` within each partition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate sample time-series data\n",
        "df_timeseries = spark.range(0, 1000000).selectExpr(\n",
        "    \"cast(id as string) as id\",\n",
        "    \"cast('2023' as string) as year\",\n",
        "    \"date_add('2023-01-01', cast(rand() * 365 as int)) as date\",\n",
        "    \"cast(rand() * 100 as double) as value\"\n",
        ")\n",
        "\n",
        "# Write to Delta table partitioned by 'year'\n",
        "df_timeseries.write.format(\"delta\").partitionBy(\"year\").mode(\"overwrite\").save(\"/tmp/timeseries_delta\")\n",
        "\n",
        "# Apply Z-Ordering on 'date' within each partition\n",
        "delta_table_timeseries = DeltaTable.forPath(spark, \"/tmp/timeseries_delta\")\n",
        "delta_table_timeseries.optimize().executeZOrderBy(\"date\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Query Example:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = spark.read.format(\"delta\").load(\"/tmp/timeseries_delta\").filter(\"date between '2023-06-01' and '2023-06-07'\").select(\"value\")\n",
        "query.explain()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Expected Output:**\n",
        "The plan will show efficient file pruning within the `year=2023` partition, enhanced by Z-Ordering on `date`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Scenario 2: Handling Skewed Data\n",
        "\n",
        "If data is skewed (e.g., some values are much more frequent), Z-Ordering can help. Here, we simulate a log dataset where `user_0` dominates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import when, rand, current_timestamp\n",
        "\n",
        "# Generate skewed log data\n",
        "df_logs = spark.range(0, 1000000).selectExpr(\n",
        "    \"cast(id as string) as log_id\",\n",
        "    \"case when rand() < 0.9 then 'user_0' else concat('user_', cast(cast(rand() * 999 as int) + 1 as string)) end as user_id\",\n",
        "    \"current_timestamp() as timestamp\"\n",
        ")\n",
        "\n",
        "# Write to Delta table\n",
        "df_logs.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/logs_delta\")\n",
        "\n",
        "# Apply Z-Ordering on 'user_id'\n",
        "delta_table_logs = DeltaTable.forPath(spark, \"/tmp/logs_delta\")\n",
        "delta_table_logs.optimize().executeZOrderBy(\"user_id\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Query Example:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = spark.read.format(\"delta\").load(\"/tmp/logs_delta\").filter(\"user_id = 'user_0'\").select(\"log_id\")\n",
        "query.explain()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Expected Output:**\n",
        "The plan will show optimized file access for `user_0`, reducing the number of files scanned despite the skew."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "In this notebook, we explored Z-Ordering in Delta Lake to optimize query performance across simple, intermediate, and advanced scenarios. We demonstrated its application on single and multiple columns, combined it with partitioning, and addressed skewed data.\n",
        "\n",
        "Key takeaways:\n",
        "- Z-Ordering improves read performance by co-locating data.\n",
        "- It comes with a write cost, so use it on frequently filtered columns.\n",
        "- Combine with partitioning for large datasets.\n",
        "\n",
        "Try applying Z-Ordering to your own data and explore the Delta Lake documentation for more!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
